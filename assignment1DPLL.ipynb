{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import norm,uniform,multivariate_normal\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1. Implement at least one standard distribution (e.g. Dirichlet, Binomial, Gaussian etc.) (PMF/PDF) from scratch (using the formula, without libraries). Create at least 4 interesting plots for the PMF/PDF and 4 plots for the likelihood function. Analyze and reflect what they mean.\n",
    "\n",
    "Gaussian distribution is the one that makes minimal assumption about a given data with a specific mean and standard deviation. Its formula comes from maximizing the entropy of a constraint distribution:\n",
    "\n",
    "$$p(x)= \\frac{1}{\\sqrt{(2\\pi)^{N}|\\Sigma|}}e^{-\\frac{(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}{2}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianDistribution(mean,sigma,boundary):\n",
    "    precision=lambda sigma: np.linalg.inv(sigma)\n",
    "    gaussian=lambda x1,x2: np.exp(-([x1, x2]-mean).T\\\n",
    "                                    @ precision(sigma)@([x1, x2]-mean))\\\n",
    "                                    /2/np.pi/np.sqrt(np.linalg.det(sigma))\n",
    "\n",
    "    dim=50\n",
    "    x = np.linspace(-boundary, boundary, dim)\n",
    "    y = np.linspace(-boundary, boundary, dim)\n",
    "    Z=np.zeros((dim,dim))\n",
    "    for i,xi in enumerate(x):\n",
    "        for j,yi in enumerate(y):\n",
    "            Z[i,j] = gaussian(xi, yi)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    return X, Y, Z\n",
    "\n",
    "boundary=2\n",
    "mean=np.array([0,0])\n",
    "sigma = np.array([[1,0],[0,1]])\n",
    "X, Y, Z=gaussianDistribution(mean,sigma,boundary)\n",
    "plt.figure()\n",
    "plt.contour(X, Y, Z.T, colors='black');\n",
    "plt.title('Standard Multivariate Gaussian')\n",
    "plt.xlabel('1-st dim')\n",
    "plt.ylabel('2-nd dim')\n",
    "plt.show()\n",
    "\n",
    "boundary=3\n",
    "mean=np.array([0,0])\n",
    "sigma = np.array([[4,0],[0,2]])\n",
    "X, Y, Z=gaussianDistribution(mean,sigma,boundary)\n",
    "plt.figure()\n",
    "plt.contour(X, Y, Z.T, colors='black');\n",
    "plt.title('Non-Standard Multivariate Gaussian\\n Diagonal covariance matrix')\n",
    "plt.xlabel('1-st dim')\n",
    "plt.ylabel('2-nd dim')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "boundary=3\n",
    "mean=np.array([0,0])\n",
    "sigma = np.array([[4,2],[2,2]])\n",
    "X, Y, Z=gaussianDistribution(mean,sigma,boundary)\n",
    "plt.figure()\n",
    "plt.contour(X, Y, Z.T, colors='black');\n",
    "plt.title('Non-Standard Multivariate Gaussian\\n  Positively correlated random-values')\n",
    "plt.xlabel('1-st dim')\n",
    "plt.ylabel('2-nd dim')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "boundary=3\n",
    "mean=np.array([0,0])\n",
    "sigma = np.array([[4,-1],[-1,2]])\n",
    "X, Y, Z=gaussianDistribution(mean,sigma,boundary)\n",
    "plt.figure()\n",
    "plt.contour(X, Y, Z.T, colors='black');\n",
    "plt.title('Non-Standard Multivariate Gaussian\\n  Negatively correlated random-values')\n",
    "plt.xlabel('1-st dim')\n",
    "plt.ylabel('2-nd dim')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The likelihood function for Gaussian \n",
    "Instead of working directly with the likelihood, we try to work with the log-likelihood for better numerical stability and easy computations.\n",
    "The log-likelihood for both mean and covariance matrix is:\n",
    "\n",
    "$$L(\\mu,\\Sigma)=-\\frac{(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}{2}-\\frac{1}{2}\\{N*ln(2\\pi)+ln(|\\Sigma|)\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanLikelihood(data,sigma,boundary):\n",
    "    precision=lambda sigma: np.linalg.inv(sigma)\n",
    "    likelihood_mu=lambda mu_x,mu_y,sigma: -(data-np.asarray([mu_x,mu_y]).T)\\\n",
    "                                        @ precision(sigma)\\\n",
    "                                        @ (data-np.asarray([mu_x,mu_y]).T).T/2\n",
    "    dim=50\n",
    "    mu_x = np.linspace(-boundary, boundary, dim)\n",
    "    mu_y = np.linspace(-boundary, boundary, dim)\n",
    "    Z=np.zeros((dim,dim))\n",
    "    for i,xi in enumerate(mu_x):\n",
    "        for j,yi in enumerate(mu_y):\n",
    "            likehood_matrix=likelihood_mu(xi, yi,sigma)\n",
    "            sum_diagonal=np.einsum('ii', likehood_matrix)\n",
    "            Z[i,j] = (likehood_matrix.ravel().sum()+sum_diagonal)/2\n",
    "    X, Y = np.meshgrid(mu_x, mu_y)\n",
    "    return X, Y, Z\n",
    "\n",
    "boundary=3\n",
    "data = np.random.rand(300,2)\n",
    "X, Y, Z=meanLikelihood(data,sigma,boundary)\n",
    "plt.figure()\n",
    "plt.contour(X, Y, Z.T, colors='black');\n",
    "plt.title('Log-likelihood for the mean $\\mu_{x}$ and $\\mu_{y}$')\n",
    "plt.xlabel('$Mean-\\mu_{x}$')\n",
    "plt.ylabel('$Mean-\\mu_{y}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covarianceLikelihood(data,mean,boundary):\n",
    "    precision=lambda sigma: np.linalg.inv(sigma)\n",
    "    likelihood_covariance=lambda mean,sigma: -(data-mean)\\\n",
    "                                        @ precision(sigma)\\\n",
    "                                        @ (data-mean).T/2\n",
    "    dim=50\n",
    "    sigma_x = np.linspace(-boundary, boundary, dim)\n",
    "    sigma_y = np.linspace(-boundary, boundary, dim)\n",
    "    Z=np.zeros((dim,dim))\n",
    "    for i,xi in enumerate(sigma_x):\n",
    "        for j,yi in enumerate(sigma_y):\n",
    "            sigma=np.diag([xi,yi])\n",
    "            likehood_matrix=likelihood_covariance(mean,sigma)\n",
    "            sum_diagonal=np.einsum('ii', likehood_matrix)\n",
    "            Z[i,j] = (likehood_matrix.ravel().sum()+sum_diagonal)/2\n",
    "    X, Y = np.meshgrid(sigma_x, sigma_y)\n",
    "    return X, Y, Z\n",
    "\n",
    "boundary=1\n",
    "data = np.random.rand(300,2)\n",
    "mean = np.array([[0,0]])\n",
    "X, Y, Z=covarianceLikelihood(data,mean,boundary)\n",
    "plt.figure()\n",
    "plt.contour(X, Y, Z.T, colors='black');\n",
    "plt.title('Log-likelihood for the variance $\\sigma_{x}$ and $\\sigma_{y}$')\n",
    "plt.xlabel('$\\sigma_{x}$')\n",
    "plt.ylabel('$\\sigma_{y}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2. Explain the key ideas of the two distributions Gamma and Poisson. In the context of conjugate priors, prove the formulas that compute the posterior hyper parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma and the Poisson distribution can form a conjugancy while the former is the prior and the latter describing the likelihood of the data given the target hyper-parameter.\n",
    "\n",
    "Gamma distribution for hyper-parameter $\\lambda$ for a given $\\alpha$ and $\\beta$ is:\n",
    "\n",
    "$$p(\\lambda|\\alpha,\\beta)=\\frac{\\beta^{-\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta\\lambda}$$\n",
    "\n",
    "The likelihood of the of the hyper-parameter $\\lambda$ for a given set of independent data $x_{1},x_{2},...,x_{N}$ using a Poisson distribution is:\n",
    "\n",
    "$$P(x_{1},x_{2},...,x_{N}|\\lambda)=\\prod_{i=1}^{N}\\frac{\\lambda^{x_{i}}e^{-\\lambda}}{x_{i}!}=\\frac{\\lambda^{\\sum_{i=1}^{N}x_{i}}e^{\\sum_{i=1}^{N}(-\\lambda)}}{\\prod_{i=1}^{N}x_{i}!}=\\frac{\\lambda^{Nx_{mu}}e^{-N\\lambda}}{\\prod_{i=1}^{N}x_{i}!}$$\n",
    "\n",
    "Using Bayes it is possible to get the update on $\\lambda$ as:\n",
    "\n",
    "$$p(\\lambda|x_{1},x_{2},...,x_{N})=\\frac{p(x_{1},x_{2},...,x_{N}|\\lambda)p(\\lambda)}{\\int p(x_{1},x_{2},...,x_{N},\\lambda)d\\lambda}=\\frac{p(x_{1},x_{2},...,x_{N}|\\lambda)p(\\lambda)}{p(x_{1},x_{2},...,x_{N})}$$\n",
    "\n",
    "Since the evidence is not dependent on $\\lambda$ it is not important on the maximum a posterior:\n",
    "\n",
    "$$p(\\lambda|x_{1},x_{2},...,x_{N})\\propto p(x_{1},x_{2},...,x_{N}|\\lambda)p(\\lambda)$$\n",
    "\n",
    "$$p(\\lambda|x_{1},x_{2},...,x_{N})\\propto \\frac{\\lambda^{Nx_{mu}}e^{-N\\lambda}}{\\prod_{i=1}^{N}x_{i}!}p(\\lambda)$$\n",
    "\n",
    "\n",
    "$$p(\\lambda|x_{1},x_{2},...,x_{N})\\propto \\frac{\\lambda^{Nx_{mu}}e^{-N\\lambda}}{\\prod_{i=1}^{N}x_{i}!}\\frac{\\beta^{-\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta\\lambda}$$\n",
    "\n",
    "\n",
    "$$p(\\lambda|x_{1},x_{2},...,x_{N})\\propto \\frac{\\beta^{-\\alpha}}{\\Gamma(\\alpha) \\prod_{i=1}^{N}x_{i}!}  \\lambda^{Nx_{mu}}e^{-N\\lambda}\\lambda^{\\alpha-1}e^{-\\beta\\lambda}$$\n",
    "\n",
    "\n",
    "$$p(\\lambda|x_{1},x_{2},...,x_{N})\\propto \\frac{\\beta^{-\\alpha}}{\\Gamma(\\alpha) \\prod_{i=1}^{N}x_{i}!}  \\lambda^{Nx_{mu}+\\alpha-1}e^{-N\\lambda-\\beta\\lambda}$$\n",
    "\n",
    "Since $\\frac{\\beta^{-\\alpha}}{\\Gamma(\\alpha) \\prod_{i=1}^{N}x_{i}!}$  is not dependent on $\\lambda$ can be removed away from the right hand side.\n",
    "\n",
    "$$p(\\lambda|x_{1},x_{2},...,x_{N})\\propto   \\lambda^{Nx_{mu}+\\alpha-1}e^{-\\alpha(N+\\beta)}$$\n",
    "\n",
    "\n",
    "$$p(\\lambda|x_{1},x_{2},...,x_{N})\\propto Gamma(Nx_{mu}+\\alpha,N+\\beta)$$\n",
    "\n",
    "Hence the Poisson and the Gamma distribution form a conjugacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3. Design an interesting, simple, multimodal distribution with two dimensions. Assume that you can evaluated from this distribution, but you cannot directly sample from it. Implement the MetropolisHastings algorithm, and visualize on top of a contour plot the steps taken by the algorithm. Clearly show when steps are accepted or rejected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain Monte-Carlo using Metropolis-Hasting sampling technique\n",
    "\n",
    "This is a sampling method that utilizes a proposal method to generate samples such as it elliviates the curse of dimensionality by trying to generate samples from high volume of likelihoods.\n",
    "Instead of drawing samples from the target distribution $\\hat{p}(z)$ this method draws samples from a proposal distribution $q(z)$ and computes the ratio $$R(z^*,z^{(\\tau)})=\\frac{\\hat{p}(z^*)q(z^{(\\tau)}|z^*)}{\\hat{p}(z^{(\\tau)})q(z^*|z^{(\\tau)})}$$\n",
    "\n",
    "The acceptation ratio is then upperbounded by one:\n",
    "$$A(z^*,z^{(\\tau)})=min(1,R(z^*,z^{(\\tau)}))=min(1,\\frac{\\hat{p}(z^*)q(z^{(\\tau)}|z^*)}{\\hat{p}(z^{(\\tau)})q(z^*|z^{(\\tau)})})$$\n",
    "\n",
    "Unlike Metropolis algorithm which requires symmetric distribution $q(z^{(\\tau)}|z^*)=q(z^*|z^{(\\tau)})$ Hasting propose the utilization of non-symmetrical ones.\n",
    "\n",
    "Eventually the sample $z^{(\\tau)}$ is accepted over $z^*$ if $A(z^*,z^{(\\tau)})>u$ where $u\\sim U[0,1]$ otherwise we reject $z^{(\\tau)}$ and keep $z^*$.\n",
    "\n",
    "Bellow is an example while sampling over the 2D distribution \n",
    "$$p(x_1,x_2)=e^{-\\frac{(2*x_{1}+sin(2\\pi*x_{1}))^2}{2}}\\frac{1}{\\sqrt{2*\\pi*0.1}}e^{-\\frac{(x2-x1^3)^2}{0.1}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibbs sampling applied on a 2D distribution as in the figure bellow \n",
    "# where one of the dimensions uses Metropolis-Hastings\n",
    "\n",
    "pi_1=lambda x1 : np.exp(-((2*x1+np.sin(6.28*x1))**2)/2)\n",
    "pi_2=lambda x1, x2 : norm.pdf(x2,x1**3,0.1)\n",
    "pi_1_2=lambda x1, x2 : np.exp(-((2*x1+np.sin(6.28*x1))**2)/2)*norm.pdf(x2,x1**3,0.1)\n",
    "pi_1_2(1,2)\n",
    "\n",
    "x = np.linspace(-1, 1, 50)\n",
    "y = np.linspace(-1, 1, 40)\n",
    "\n",
    "X1, X2 = np.meshgrid(x, y)\n",
    "Z = pi_1_2(X1, X2)\n",
    "\n",
    "plt.contour(X1, X2, Z, colors='black');\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbsMetropolisHasting(sigma=1,\n",
    "                           nr_iterations=1000):\n",
    "    x1=[]\n",
    "    x2=[]\n",
    "    x1_reject=[]\n",
    "    x2_reject=[]\n",
    "    x1.append(0)\n",
    "    x2.append(0)\n",
    "    \n",
    "    \n",
    "    x1_reject.append(0)\n",
    "    x2_reject.append(0)\n",
    "    for i in (range(nr_iterations)):\n",
    "        \n",
    "        # Sampling first dimension\n",
    "        u=np.random.uniform(0,1,1)\n",
    "        x1_temp=np.random.normal(x1[-1],sigma**2,1)\n",
    "        ratio=pi_1_2(x1_temp,x2[i])/pi_1_2(x1[i],x2[i])\n",
    "        a=np.min((1,ratio))\n",
    "        if u<a:\n",
    "            x1.append(x1_temp)\n",
    "        else:\n",
    "            x1_reject.append(x1_temp)\n",
    "            x2_reject.append(x2[-1])\n",
    "            x1.append(x1[-1])\n",
    "        \n",
    "        \n",
    "        # Sampling second dimension\n",
    "        u=np.random.uniform(0,1,1)\n",
    "        x2_temp=np.random.normal(x2[-1],sigma**2,1)\n",
    "        ratio=pi_1_2(x1[-1],x2_temp)/pi_1_2(x1[-1],x2[-1])\n",
    "        a=np.min((1,ratio))\n",
    "        if u<a:\n",
    "            x2.append(x2_temp)\n",
    "        else:\n",
    "            x1_reject.append(x1[-1])\n",
    "            x2_reject.append(x2_temp)\n",
    "            x2.append(x2[-1])\n",
    "            \n",
    "        clear_output(True)\n",
    "        plt.figure()\n",
    "        plt.scatter(x1,x2)\n",
    "        plt.plot(x1,x2)\n",
    "        plt.contour(X1, X2, Z, colors='black')\n",
    "        plt.xlabel('x1')\n",
    "        plt.ylabel('x2')\n",
    "        plt.title(\"Gibbs with Metropolis Hasting samples \"+str(i)+\"/\"+str(nr_iterations))\n",
    "        plt.show()\n",
    "\n",
    "    return x1,x2,x1_reject,x2_reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_,x2_,x1_reject,x2_reject=gibbsMetropolisHasting(.5,100)\n",
    "\n",
    "clear_output(True)\n",
    "plt.close('all')\n",
    "plt.figure()\n",
    "plt.scatter(x1_,x2_)\n",
    "plt.contour(X1, X2, Z, colors='black')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title(\"Gibbs with Metropolis Hasting accepted samples\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x1_reject,x2_reject)\n",
    "plt.contour(X1, X2, Z, colors='black')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title(\"Gibbs with Metropolis Hasting rejected samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain Monte-Carlo using Hamiltonian Dynamics\n",
    "###### Metripolis Hasting algorithm can stuck into one of the modals of the distribution and not expling the rest of the modes.\n",
    "###### One way to circumvent this my heuristically adjusting the step-size using the spread of the proptoosal distribution. However too large step size leads to a large number of rejections, while a too small step-size makes makes the exploration too slow.\n",
    "###### in high-dimenbsional space the exploration is nenarly a random-walk behavior thus the exploration is sup-optimal.\n",
    "\n",
    "To mitigate these drawbacks Hamitonian Monte-Carlo (HMC) utilizes the target distribution and the laws of dynamics in mechanical physics to design adaptive step-size for the proptoosed samples.\n",
    "\n",
    "The target distribution p(z) is then a modeled using the Gibbs canonical distribution from statistical mechanics as\n",
    "$$p(z)\\propto e^{\\frac{-U(z)}{T}} $$ where T is the temperature and U(z) is the energy of the state for the particle at state z.\n",
    "\n",
    "Apart from the potential energy U(z) this method introduces an additional auxilliary  component kinetic energy K(v) that is dependent on the speed (v) as auxilliary variable.\n",
    "\n",
    "Eventually the total mechanical energy is:\n",
    "$$E(z,v)=U(z)+K(v),s.t: K(v)=\\sum_{i}\\frac{v_{i}^2}{2}$$\n",
    "\n",
    "The state distribution of the particles is then dependent on the total energy as:\n",
    "\n",
    "$$p(z,v)\\propto e^{\\frac{-E(z,v)}{T}}=e^{\\frac{-U(z)}{T}}e^{\\frac{-K(v)}{T}}\\propto p(z)p(v)$$\n",
    "\n",
    "#### The physicial dynamics of the target distribution through Hamiltonian\n",
    "\n",
    "In order to sample multiple different positions of the samples inside the energy well defined from E(z,v) we utilize these two physics equations:\n",
    "$$\\frac{\\partial z_{i}(t)}{\\partial t}=\\frac{\\partial E(z,v)}{\\partial v_{i}}=\\frac{\\partial K(z)}{\\partial v_{i}}$$\n",
    "$$m\\frac{\\partial v_{i}(t)}{\\partial t}=-\\frac{\\partial E(z,v)}{\\partial z_{i}}=-\\frac{\\partial U(z)}{\\partial z_{i}} $$\n",
    "\n",
    "Since the energy of the closed system is preserved $E(z,v)=E_{0}$ it is possible to get different samples inside this target distribution while simulating particles whose statistical trajectory is guided by the two equations above.\n",
    "\n",
    "Sampling the speed (v) is quite simple as it follows a (multivariate) normal distribution:\n",
    "\n",
    "$$p(v)=e^{\\frac{-K(v)}{T}}=e^{\\frac{-\\sum_{i}mv_{i}}{2T}}=e^{\\frac{-mV^{T}V}{2T}}$$\n",
    "\n",
    "###### In a nutshell\n",
    "\n",
    "Start the sample moving with a random speed drawn from the normal distribution and stop it.\n",
    "Continue this proceedure until the sufficient number of samples have been accummulated.\n",
    "\n",
    "However, numerical solutions to the partial derivative equations (PDE) cannot be solved analytically and their numerical solution does not ensure the preservation of the energy $E(z,v)$.\n",
    "\n",
    "To mitigate this problem Metropolis Hastings rejections are employed to compensate difference in energy between energy between the start the and the stop of the particle position.\n",
    "\n",
    "Leapfrog numerical integration offers an numerical integration that is reversible in time.\n",
    "This reversability ensures the detailed balance.\n",
    "\n",
    "###### Physical analogy\n",
    "\n",
    "The trajectory of the particle that roams inside the energy well defined by the target distribution is equivalent to a classical harmonic oscilator without any dampling (conservation of energy).\n",
    "This is governt by a second had ordinary differential equation (ODE) $z^{''}+x=0$.\n",
    "To simplify the solution this is converted into two ODEs where $z^{'}=v$.\n",
    "\n",
    "$$z^{'}=v $$\n",
    "$$v^{'}=-z $$\n",
    "\n",
    "These two equations equivalues to a system with kinetic energy $K(v)=\\frac{1}{2}v^{2}$ and potential energy $U(z)=\\frac{1}{2}z^{2}$.\n",
    "In the case of multidimensial distribution:\n",
    "$$K(v)=\\frac{v^{T}v}{2}$$\n",
    "$$U(z)=\\frac{z^{T}z}{2}$$\n",
    "\n",
    "Where the acceptance rate is:\n",
    "\n",
    "$$A(z^*,z^{(\\tau)})=\\frac{e^{-U(z^*)-K(z^*)}}{e^{-U(z^{(\\tau)})-K(z^{(\\tau)})}}=e^{U(z^{(\\tau)})-U(z^*)+K(z^{(\\tau)})-K(z^*)}$$\n",
    "\n",
    "where the ODEs $z^{'}=v $, $v^{'}=-x$ can be organised into a matrix formation as $z^{'}=Ax$ where:\n",
    "\n",
    "$A=\\begin{bmatrix}\n",
    "0 & 1\\\\\n",
    "-1 & 0\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Since the eigenvalues of the matrix $A$ are $i$ and $-i$ the solution of position $x(t)=e^{it}$.\n",
    "Equation for $x(t)$ is just a circle that does not changes its shape.\n",
    "\n",
    "###### Euler solution to ODE\n",
    "\n",
    "Numerical solution to the ODE $z^{'}=Ax$ is from Euler where $\\frac{z_{n+1}-z_{n}}{\\Delta t}=Az_{n}\\to z_{n+1}=z_{n}+\\Delta tAz_{n}=(I+\\Delta tA)z_{n}=Bz_{n};s.t:B=I+\\Delta tA$\n",
    "\n",
    "###### Leapfrog solution to ODE\n",
    "Instead of performing the updates simultaneosly, leapfrog method splits this across variables.\n",
    "It makes one half-step towards the first variable.\n",
    "Makes a full step towards the second variable using the updated first variable.\n",
    "Takes one final half step for the first variable using the updated second variable.\n",
    "\n",
    "\n",
    "###### Concrete example for HMC execution\n",
    "\n",
    "Consider the target distribution is a multivariate Gaussian with zero mean and covariance $\\Sigma$ such as $z\\sim N(o,\\Sigma)$ and:\n",
    "\n",
    "$$U(z)=\\frac{z^{T}\\Sigma^{-1}z}{2}$$\n",
    "\n",
    "The matrix A in this case is:\n",
    "\n",
    "\n",
    "$A=\\begin{bmatrix}\n",
    "0 & 1\\\\\n",
    "-\\Sigma^{-1} & 0\n",
    "\\end{bmatrix} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=np.array([0,0])\n",
    "sigma = np.array([[4,1],[1,4]])\n",
    "precision=lambda cov: np.linalg.inv(sigma)\n",
    "gaussian=lambda x1,x2: np.exp(-([x1, x2]-mean).T\\\n",
    "                                @ precision(sigma)@([x1, x2]-mean))\\\n",
    "                                /2/np.pi/np.sqrt(np.linalg.det(sigma))\n",
    "\n",
    "dim=50\n",
    "boundary=5\n",
    "x = np.linspace(-boundary, boundary, dim)\n",
    "y = np.linspace(-boundary, boundary, dim)\n",
    "Z=np.zeros((dim,dim))\n",
    "for i,xi in enumerate(x):\n",
    "    for j,yi in enumerate(y):\n",
    "        Z[i,j] = gaussian(xi, yi)\n",
    "\n",
    "\n",
    "X1, X2 = np.meshgrid(x, y)\n",
    "\n",
    "plt.figure()\n",
    "plt.contour(X1,X2, Z.T, colors='black');\n",
    "plt.title('Multivariate Gaussian as target distribution')\n",
    "plt.xlabel('1-st dim')\n",
    "plt.ylabel('2-nd dim')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leapFrog(sigma,z,v,step, nr_dimensions):\n",
    "    v=v-step/2*la.inv(sigma)@z\n",
    "    z_trajectory=[]\n",
    "    for i in range(nr_dimensions-1):\n",
    "        z=z+step*v\n",
    "        v=v-step*la.inv(sigma)@z\n",
    "        z_trajectory.append(z)\n",
    "        \n",
    "\n",
    "    z=z+step*v\n",
    "    v=v-step/2*la.inv(sigma)@z\n",
    "    z_trajectory.append(z)\n",
    "    z_trajectory=np.asarray(z_trajectory)\n",
    "    \n",
    "    \n",
    "    return z_trajectory,z,v\n",
    "\n",
    "def energyRatio(sigma,z_start,v_start,z_stop,v_stop):\n",
    "    energy_start=z_start@la.inv(sigma)@z_start+v_start@v_start\n",
    "    energy_stop=z_stop@la.inv(sigma)@z_stop+v_stop@v_stop\n",
    "    energy_ratio=energy_stop-energy_start\n",
    "    return energy_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_iterations = 1000\n",
    "integration_steps = 0.01\n",
    "nr_integration_steps = 100\n",
    "\n",
    "samples_positioning = np.zeros((nr_iterations+1, 2))\n",
    "\n",
    "# starting position for the particle in the center of the space\n",
    "z_start = np.array([-0,0])\n",
    "samples_positioning[0] = z_start\n",
    "for i in range(nr_iterations+1):\n",
    "    # Draw a random velocity\n",
    "    v_start = np.random.normal(0,1,2)\n",
    "\n",
    "    # Integrate the trajectory of the particle\n",
    "    z_trajectory,z_stop, v_stop = leapFrog(sigma, z_start, v_start, \n",
    "                                 integration_steps, \n",
    "                                 nr_integration_steps)\n",
    "    \n",
    "    z_start = samples_positioning[i]\n",
    "    \n",
    "    # Acceptance ratio\n",
    "    a = np.exp(-energyRatio(sigma, z_start, v_start, z_stop, v_stop))\n",
    "    \n",
    "    # Metropolis-Hasting accept-reject\n",
    "    r = np.random.rand()\n",
    "    if r < a:\n",
    "        samples_positioning[i+1] = z_stop\n",
    "    else:\n",
    "        samples_positioning[i+1] = z_start\n",
    "    \n",
    "    \n",
    "    clear_output(True)\n",
    "    plt.figure()\n",
    "    plt.contour(X1,X2, Z.T, colors='black');\n",
    "    plt.plot(z_trajectory[:,0], z_trajectory[:,1],'b')\n",
    "    plt.plot(z_trajectory[:,0], z_trajectory[:,1],'bx')\n",
    "    plt.title(\"Multivariate Gaussian as target distribution\\n Iteration \"+str(i)+\"/\"+str(nr_iterations))\n",
    "    plt.xlabel('1-st dim')\n",
    "    plt.ylabel('2-nd dim')\n",
    "    plt.xlim([-boundary, boundary])\n",
    "    plt.ylim([-boundary, boundary])\n",
    "    plt.show()\n",
    "    \n",
    "    clear_output(True)\n",
    "    plt.figure()\n",
    "    plt.contour(X1,X2, Z.T, colors='black');\n",
    "    plt.scatter(samples_positioning[1:,0], \n",
    "                samples_positioning[1:,1],  \n",
    "                c=np.arange(nr_iterations)[::-1], \n",
    "                cmap='Reds')\n",
    "    plt.title(\"Multivariate Gaussian as target distribution\\n Iteration \"+str(i)+\"/\"+str(nr_iterations))\n",
    "    plt.xlabel('1-st dim')\n",
    "    plt.ylabel('2-nd dim')\n",
    "    plt.xlim([-boundary, boundary])\n",
    "    plt.ylim([-boundary, boundary])\n",
    "    plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
