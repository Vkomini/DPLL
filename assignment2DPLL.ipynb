{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6068d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.linalg as la\n",
    "import torch.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from IPython.display import clear_output\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68559bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias=4\n",
    "slope=3\n",
    "\n",
    "nr_points=20\n",
    "\n",
    "mean_noise=0.0\n",
    "variance_noise=20.0\n",
    "\n",
    "\n",
    "# create dummy data for training\n",
    "x_values=[i for i in range(nr_points)]\n",
    "x_train=np.array(x_values, dtype=np.float32)\n",
    "x_train=x_train.reshape(-1,1)\n",
    "\n",
    "y_values=[slope*i+bias+np.random.normal(loc=mean_noise, scale=variance_noise,size=1) for i in range(nr_points)]\n",
    "y_train=np.array(y_values,dtype=np.float32)\n",
    "y_train=y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d88c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Linear Regressor in Pytorch\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.linear=torch.nn.Linear(input_size,output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y=self.linear(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=1   # takes variable 'x'\n",
    "output_dim=1  # takes variable 'y'\n",
    "learning_rate=0.00001\n",
    "epochs=1000\n",
    "\n",
    "model = LinearRegression(input_dim, output_dim)\n",
    "\n",
    "criterion=torch.nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    inputs=Variable(torch.from_numpy(x_train))\n",
    "    labels=Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs=model(inputs)\n",
    "    loss=criterion(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "    clear_output(True)\n",
    "    print('epoch {}, loss {}'.format(epoch,loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c867b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): \n",
    "    predicted=model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    \n",
    "plt.clf()\n",
    "plt.plot(x_train,y_train,'go',label='True data',alpha=0.5)\n",
    "plt.plot(x_train,predicted,'--',label='Predictions',alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8393f0",
   "metadata": {},
   "source": [
    "### Markov Chain Monte-Carlo using Hamiltonian Dynamics\n",
    "\n",
    "###### Metripolis Hasting algorithm can stuck into one of the modals of the distribution and not expling the rest of the modes.\n",
    "###### One way to circumvent this my heuristically adjusting the step-size using the spread of the proptoosal distribution. However too large step size leads to a large number of rejections, while a too small step-size makes makes the exploration too slow.\n",
    "###### In high-dimensional space the exploration is nenarly a random-walk behavior thus the exploration is sup-optimal.\n",
    "\n",
    "To mitigate these drawbacks Hamitonian Monte-Carlo (HMC) utilizes the target distribution and the laws of dynamics in mechanical physics to design adaptive step-size for the proptoosed samples.\n",
    "\n",
    "The target distribution p(\\omega) is then a modeled using the Gibbs canonical distribution from statistical mechanics as\n",
    "$$p(\\omega)\\propto e^{\\frac{-U(\\omega)}{T}} $$ where T is the temperature and U(\\omega) is the energy of the state for the particle at state z.\n",
    "\n",
    "Apart from the potential energy U(\\omega) this method introduces an additional auxilliary  component kinetic energy K(v) that is dependent on the speed (v) as auxilliary variable.\n",
    "\n",
    "Eventually the total mechanical energy is:\n",
    "$$E(z,v)=U(\\omega)+K(v),s.t: K(v)=\\sum_{i}\\frac{v_{i}^2}{2}$$\n",
    "\n",
    "The state distribution of the particles is then dependent on the total energy as:\n",
    "\n",
    "$$p(z,v)\\propto e^{\\frac{-E(z,v)}{T}}=e^{\\frac{-U(\\omega)}{T}}e^{\\frac{-K(v)}{T}}\\propto p(\\omega)p(v)$$\n",
    "\n",
    "#### The physicial dynamics of the target distribution through Hamiltonian\n",
    "\n",
    "In order to sample multiple different positions of the samples inside the energy well defined from E(z,v) we utilize these two physics equations:\n",
    "$$\\frac{\\partial \\omega(t)}{\\partial t}=\\frac{\\partial E(z,v)}{\\partial v}=\\frac{\\partial K(v)}{\\partial v}$$\n",
    "$$m\\frac{\\partial v(t)}{\\partial t}=-\\frac{\\partial E(z,v)}{\\partial \\omega}=-\\frac{\\partial U(\\omega)}{\\partial \\omega} $$\n",
    "\n",
    "Since the energy of the closed system is preserved $E(z,v)=E_{0}$ it is possible to get different samples inside this target distribution while simulating particles whose statistical trajectory is guided by the two equations above.\n",
    "\n",
    "Sampling the speed (v) is quite simple as it follows a (multivariate) normal distribution:\n",
    "\n",
    "$$p(v)=e^{\\frac{-K(v)}{T}}=e^{\\frac{-\\sum_{i}mv_{i}}{2T}}=e^{\\frac{-mV^{T}V}{2T}}$$\n",
    "\n",
    "###### In a nutshell\n",
    "\n",
    "Start the sample moving with a random speed drawn from the normal distribution and stop it.\n",
    "Continue this proceedure until the sufficient number of samples have been accummulated.\n",
    "\n",
    "However, numerical solutions to the partial derivative equations (PDE) cannot be solved analytically and their numerical solution does not ensure the preservation of the energy $E(z,v)$.\n",
    "\n",
    "To mitigate this problem Metropolis Hastings rejections are employed to compensate difference in energy between energy between the start the and the stop of the particle position.\n",
    "\n",
    "Leapfrog numerical integration offers an numerical integration that is reversible in time.\n",
    "This reversability ensures the detailed balance.\n",
    "\n",
    "###### Physical analogy\n",
    "\n",
    "The trajectory of the particle that roams inside the energy well defined by the target distribution is equivalent to a classical harmonic oscilator without any dampling (conservation of energy).\n",
    "This is governt by a second had ordinary differential equation (ODE) $z^{''}+z=0$.\n",
    "To simplify the solution this is converted into two ODEs where $z^{'}=v$.\n",
    "\n",
    "In our case:\n",
    "\n",
    "$$U(\\omega)=\\frac{(y-f_{\\omega}(x))^{T}(y-f_{\\omega}(x))}{2\\sigma}$$\n",
    "$$K(v)=-\\frac{V^{T}V}{2}$$\n",
    "\n",
    "$$\\frac{\\partial \\omega(t)}{\\partial t}=\\frac{\\partial K(v)}{\\partial V}=-V$$\n",
    "$$\\frac{\\partial v(t)}{\\partial t}=-\\frac{\\partial U(\\omega)}{\\partial \\omega}=-\\frac{\\partial f_{\\omega}(x)}{\\partial \\omega}(y-f_{\\omega}(x))$$\n",
    "\n",
    "\n",
    "###### Euler solution to ODE\n",
    "As a result the Euler solution to this PDE are:\n",
    "\n",
    "$$\\frac{\\omega(t+\\Delta t)-\\omega(t)}{\\Delta t}=\\frac{\\partial K(v)}{\\partial v} \\to \\omega(t+\\Delta t)=\\omega(t)+\\Delta t \\frac{\\partial K(v)}{\\partial v}$$\n",
    "$$\\frac{v(t+\\Delta t)-v(t)}{\\Delta t}=-\\frac{\\partial U(\\omega)}{\\partial \\omega} \\to v(t+\\Delta t)=v(t) -\\Delta t*\\frac{\\partial U(\\omega)}{\\partial \\omega} $$\n",
    "\n",
    "###### Leapfrog solution to ODE\n",
    "\n",
    "Instead of performing the updates simultaneosly, leapfrog method splits this across variables.\n",
    "It makes one half-step towards the first variable.\n",
    "Makes a full step towards the second variable using the updated first variable.\n",
    "Takes one final half step for the first variable using the updated second variable.\n",
    "\n",
    "Leapfrog integration:\n",
    "\n",
    "$$v(t+\\frac{\\Delta t}{2})=v(t) -\\frac{\\Delta t}{2}\\frac{\\partial U(\\omega)}{\\partial \\omega}$$\n",
    "$$\\omega(t+\\Delta t)=\\omega(t)-\\Delta t *\\frac{\\partial K(v)}{\\partial v}=\\omega(t)+\\Delta t*\\frac{\\partial K(v)}{\\partial v}$$\n",
    "$$v(t+\\frac{\\Delta t}{2})=v(t) -\\frac{\\Delta t}{2}\\frac{\\partial U(\\omega)}{\\partial \\omega}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=50\n",
    "boundary=10\n",
    "x = np.linspace(-boundary, boundary, dim)\n",
    "y = np.linspace(-boundary, boundary, dim)\n",
    "Z=np.zeros((dim,dim))\n",
    "for i,w in enumerate(x):\n",
    "    for j,b in enumerate(y):\n",
    "        model.linear.weight.data=torch.tensor(w).float().reshape(1,1)\n",
    "        model.linear.bias.data=torch.tensor(b).float() .reshape(1,1)\n",
    "        outputs=model(inputs)\n",
    "        Z[i,j]=torch.exp(-criterion(outputs,labels)/200)\n",
    "\n",
    "\n",
    "X1, X2 = np.meshgrid(x, y)\n",
    "\n",
    "plt.figure()\n",
    "plt.contour(X1,X2, Z.T, colors='black');\n",
    "plt.axhline(y=bias, color='r', linestyle='-')\n",
    "plt.axvline(x=slope, color='r', linestyle='-')\n",
    "plt.title('Energy landscape')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Bias')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63025685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dUdw(labels,inputs,w,b):\n",
    "    model.linear.weight.data, model.linear.bias.data=w,b\n",
    "    optimizer.zero_grad()\n",
    "    outputs=model(inputs)\n",
    "    loss=criterion(outputs,labels)\n",
    "    loss.backward()\n",
    "    return  model.linear.weight.grad, model.linear.bias.grad\n",
    "def leapFrog(w_start,b_start,v_w,v_b,DEBUG=True):\n",
    "    #####LeapFrog Integration#####\n",
    "    w_trajectory=[]\n",
    "    b_trajectory=[]\n",
    "    \n",
    "    w_gradient=[]\n",
    "    b_gradient=[]\n",
    "    \n",
    "    v_w_speed=[]\n",
    "    v_b_speed=[]\n",
    "    \n",
    "    w_trajectory.append(w_start)\n",
    "    b_trajectory.append(b_start)\n",
    "    \n",
    "    # Get the gradient of the curvature at the given weight and biases\n",
    "    d_w,d_b=dUdw(labels,inputs,w_start,b_start)\n",
    "\n",
    "    w_gradient.append(d_w)\n",
    "    b_gradient.append(d_b)\n",
    "    \n",
    "    # Accumulated speed at half step\n",
    "    v_w-=d_w.squeeze()*integration_steps/2\n",
    "    v_b-=d_b.squeeze()*integration_steps/2\n",
    "\n",
    "    v_w_speed.append(v_w.item())\n",
    "    v_b_speed.append(v_b.item())\n",
    "    \n",
    "    for j in range(nr_integration_steps+1):\n",
    "        \n",
    "        w_start+=integration_steps*v_w\n",
    "        b_start+=integration_steps*v_b\n",
    "        \n",
    "        w_trajectory.append(w_start.item())\n",
    "        b_trajectory.append(b_start.item())\n",
    "    \n",
    "        # Get the gradient of the curvature at the given weight and biases\n",
    "        d_w,d_b=dUdw(labels,inputs,w_start,b_start)\n",
    "\n",
    "        # Accumulated speed at half step\n",
    "        v_w-=d_w.squeeze()*integration_steps/2\n",
    "        v_b-=d_b.squeeze()*integration_steps/2\n",
    "        \n",
    "        w_gradient.append(d_w.item())\n",
    "        b_gradient.append(d_b.item())\n",
    "        \n",
    "        v_w_speed.append(v_w.item())\n",
    "        v_b_speed.append(v_b.item())\n",
    "\n",
    "        \n",
    "        if DEBUG:\n",
    "            clear_output(True)\n",
    "            plt.subplot(211)\n",
    "            plt.scatter(w_trajectory, b_trajectory)\n",
    "            plt.contour(X1,X2, Z.T, colors='black');\n",
    "            #plt.axhline(y=bias, color='r', linestyle='-')\n",
    "            #plt.axvline(x=slope, color='r', linestyle='-')\n",
    "            plt.quiver(w_trajectory[-1], b_trajectory[-1], w_gradient[-1], b_gradient[-1],color='r')\n",
    "            plt.quiver(w_trajectory[-1], b_trajectory[-1], v_w_speed[-1], v_b_speed[-1],color='b')\n",
    "            plt.xlabel('Weight w')\n",
    "            plt.ylabel('Bias b')\n",
    "            #plt.xlim([0, boundary])\n",
    "            #plt.ylim([-boundary, boundary])\n",
    "\n",
    "            with torch.no_grad(): \n",
    "                predicted=model(inputs).data.numpy()\n",
    "\n",
    "            plt.subplot(212)\n",
    "            plt.plot(x_train,y_train,'go',label='True data',alpha=0.5)\n",
    "            plt.plot(x_train,predicted,'--',label='Predictions',alpha=0.5)\n",
    "            plt.legend(loc='best')\n",
    "            plt.show()\n",
    "        \n",
    "    return w_start,b_start,v_w,v_b\n",
    "\n",
    "def energyComputation(labels,inputs,w,b):\n",
    "    model.linear.weight.data=torch.tensor(w).float().reshape(1,1).clone().detach().requires_grad_(True)\n",
    "    model.linear.bias.data=torch.tensor(b).float() .reshape(1,1).clone().detach().requires_grad_(True)\n",
    "    outputs=model(inputs)\n",
    "    return -criterion(outputs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_iterations = 100\n",
    "integration_steps = 0.01\n",
    "nr_integration_steps = 100\n",
    "\n",
    "# starting position for the particle in the center of the space\n",
    "w_start,b_start = 0.*model.linear.weight.data, 0.*model.linear.bias.data.data\n",
    "\n",
    "inputs=Variable(torch.from_numpy(x_train))\n",
    "labels=Variable(torch.from_numpy(y_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "w_rejected = []\n",
    "w_accepted = []\n",
    "\n",
    "b_rejected = []\n",
    "b_accepted = []\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(nr_iterations+1)):\n",
    "    \n",
    "    # Draw a random velocity\n",
    "    v_w_start = torch.normal(mean=torch.tensor(0.), std=torch.tensor(1.))\n",
    "    v_b_start = torch.normal(mean=torch.tensor(0.), std=torch.tensor(1.))\n",
    "    \n",
    "    energy_start=energyComputation(labels,inputs,w_start,b_start)+v_w_start**2+v_b_start**2\n",
    "    w_stop,b_stop,v_w_stop,v_b_stop=leapFrog(w_start,b_start,v_w_start,v_b_start,False)   \n",
    "    \n",
    "    energy_stop=energyComputation(labels,inputs,w_stop,b_stop)+v_w_stop**2+v_b_stop**2\n",
    "    \n",
    "    # Acceptance ratio\n",
    "    a=torch.exp(energy_start-energy_stop)\n",
    "    \n",
    "    # Metropolis-Hasting accept-reject\n",
    "    r = np.random.rand()\n",
    "    if r < a:\n",
    "        w_accepted.append(w_stop.item())\n",
    "        b_accepted.append(b_stop.item())\n",
    "\n",
    "    else:\n",
    "        w_rejected.append(w_stop.item())\n",
    "        b_rejected.append(b_stop.item())\n",
    "    \n",
    "    w_start,b_start=w_stop,b_stop\n",
    "    clear_output(True)\n",
    "    \n",
    "    #plt.figure()\n",
    "    plt.subplot(311)\n",
    "            \n",
    "    plt.contour(X1,X2, Z.T, colors='black');\n",
    "    plt.scatter(w_accepted, \n",
    "                b_accepted,  \n",
    "                c='r',\n",
    "                cmap='Reds')\n",
    "    plt.title(\"Accepted Points \\n Iteration \"+str(i)+\"/\"+str(nr_iterations))\n",
    "    plt.xlabel('Weights')\n",
    "    plt.ylabel('Bias')\n",
    "    plt.xlim([0, boundary])\n",
    "    plt.ylim([-boundary, boundary])\n",
    "    \n",
    "    plt.subplot(312)\n",
    "            \n",
    "    plt.contour(X1,X2, Z.T, colors='black');\n",
    "    plt.scatter(w_rejected, \n",
    "                b_rejected,  \n",
    "                c='r',\n",
    "                cmap='Reds')\n",
    "    plt.title(\"Rejected Points \\n Iteration \"+str(i)+\"/\"+str(nr_iterations))\n",
    "    plt.xlabel('Weights')\n",
    "    plt.ylabel('Bias')\n",
    "    plt.xlim([0, boundary])\n",
    "    plt.ylim([-boundary, boundary])\n",
    "    \n",
    "    \n",
    "    plt.subplot(313)\n",
    "    plt.plot(x_train,y_train,'go',label='True data',alpha=0.5)\n",
    "    for w,b in zip(w_accepted,b_accepted):\n",
    "        model.linear.weight.data=torch.tensor(w).float().reshape(1,1).clone().detach().requires_grad_(True)\n",
    "        model.linear.bias.data=torch.tensor(b).float().reshape(1,1).clone().detach().requires_grad_(True)\n",
    "\n",
    "        with torch.no_grad(): \n",
    "                    predicted=model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "\n",
    "        plt.plot(x_train,predicted,'--',alpha=0.5)\n",
    "    plt.plot(x_train,predicted,'--',label='Predictions',alpha=0.5)    \n",
    "    plt.show()\n",
    "    time.sleep(3)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818bf1f8",
   "metadata": {},
   "source": [
    "# Seatbelts prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Seatbelts.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_=data.to_numpy()\n",
    "x_train=data_[:,6]\n",
    "y_train=data_[:,1]\n",
    "x_train=x_train.reshape((-1,1))\n",
    "y_train=y_train.reshape((-1,1))\n",
    "plt.plot(y)\n",
    "plt.plot(x)\n",
    "plt.show()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Linear Regressor in Pytorch\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.linear=torch.nn.Linear(input_size,output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y=self.linear(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ce89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=1   # takes variable 'x'\n",
    "output_dim=1  # takes variable 'y'\n",
    "learning_rate=0.001\n",
    "epochs=10000\n",
    "\n",
    "model = LinearRegression(input_dim, output_dim)\n",
    "\n",
    "criterion=torch.nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    inputs=Variable(torch.from_numpy(x_train).float())\n",
    "    labels=Variable(torch.from_numpy(y_train).float())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs=model(inputs)\n",
    "    loss=criterion(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "    clear_output(True)\n",
    "    print('epoch {}, loss {}'.format(epoch,loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d136bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): \n",
    "    predicted=model(Variable(torch.from_numpy(x_train).float())).data.numpy()\n",
    "    \n",
    "plt.clf()\n",
    "plt.plot(x_train,y_train,'go',label='True data',alpha=0.5)\n",
    "plt.plot(x_train,predicted,'--',label='Predictions',alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=50\n",
    "boundary=200\n",
    "x = np.linspace(-8*boundary, boundary, dim)\n",
    "y = np.linspace(-0, 2*boundary, dim)\n",
    "Z=np.zeros((dim,dim))\n",
    "for i,w in enumerate(x):\n",
    "    for j,b in enumerate(y):\n",
    "        model.linear.weight.data=torch.tensor(w).float().reshape(1,1)\n",
    "        model.linear.bias.data=torch.tensor(b).float() .reshape(1,1)\n",
    "        outputs=model(inputs)\n",
    "        Z[i,j]=torch.exp(-criterion(outputs,labels)/1000)\n",
    "\n",
    "\n",
    "X1, X2 = np.meshgrid(x, y)\n",
    "\n",
    "plt.figure()\n",
    "plt.contour(X1,X2, Z.T, colors='black');\n",
    "plt.title('Energy landscape')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Bias')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fe6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_iterations = 100\n",
    "integration_steps = 0.01\n",
    "nr_integration_steps = 100\n",
    "\n",
    "# starting position for the particle in the center of the space\n",
    "w_start,b_start = 0.*model.linear.weight.data, 0.*model.linear.bias.data.data\n",
    "\n",
    "inputs=Variable(torch.from_numpy(x_train).float())\n",
    "labels=Variable(torch.from_numpy(y_train).float())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "w_rejected = []\n",
    "w_accepted = []\n",
    "\n",
    "b_rejected = []\n",
    "b_accepted = []\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(nr_iterations+1)):\n",
    "    \n",
    "    # Draw a random velocity\n",
    "    v_w_start = torch.normal(mean=torch.tensor(0.), std=torch.tensor(10.))\n",
    "    v_b_start = torch.normal(mean=torch.tensor(0.), std=torch.tensor(10.))\n",
    "    \n",
    "    energy_start=energyComputation(labels,inputs,w_start,b_start)+v_w_start**2+v_b_start**2\n",
    "    w_stop,b_stop,v_w_stop,v_b_stop=leapFrog(w_start,b_start,v_w_start,v_b_start,False)   \n",
    "    \n",
    "    energy_stop=energyComputation(labels,inputs,w_stop,b_stop)+v_w_stop**2+v_b_stop**2\n",
    "    \n",
    "    # Acceptance ratio\n",
    "    a=torch.exp(energy_stop-energy_start)\n",
    "    \n",
    "    # Metropolis-Hasting accept-reject\n",
    "    r = np.random.rand()\n",
    "    if r < a:\n",
    "        w_accepted.append(w_stop.item())\n",
    "        b_accepted.append(b_stop.item())\n",
    "\n",
    "    else:\n",
    "        w_rejected.append(w_stop.item())\n",
    "        b_rejected.append(b_stop.item())\n",
    "    \n",
    "    w_start,b_start=w_stop,b_stop\n",
    "    clear_output(True)\n",
    "    \n",
    "    #plt.figure()\n",
    "    plt.subplot(131)\n",
    "            \n",
    "    plt.contour(X1,X2, Z.T, colors='black');\n",
    "    plt.scatter(w_accepted, \n",
    "                b_accepted, \n",
    "                s=100,\n",
    "                c='r',\n",
    "                cmap='Reds')\n",
    "    plt.title(\"Accepted Points \\n Iteration \"+str(i)+\"/\"+str(nr_iterations))\n",
    "    plt.xlabel('Weights')\n",
    "    plt.ylabel('Bias')\n",
    "    #plt.xlim([0, boundary])\n",
    "    #plt.ylim([-boundary, boundary])\n",
    "    \n",
    "    plt.subplot(132)\n",
    "            \n",
    "    plt.contour(X1,X2, Z.T, colors='black');\n",
    "    plt.scatter(w_rejected, \n",
    "                b_rejected,\n",
    "                s=100,\n",
    "                c='r',\n",
    "                cmap='Reds')\n",
    "    plt.title(\"Rejected Points \\n Iteration \"+str(i)+\"/\"+str(nr_iterations))\n",
    "    plt.xlabel('Weights')\n",
    "    plt.ylabel('Bias')\n",
    "    #plt.xlim([0, boundary])\n",
    "    #plt.ylim([-boundary, boundary])\n",
    "    \n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.plot(x_train,y_train,'go',label='True data',alpha=0.5)\n",
    "    for w,b in zip(w_accepted,b_accepted):\n",
    "        model.linear.weight.data=torch.tensor(w).float().reshape(1,1).clone().detach().requires_grad_(True)\n",
    "        model.linear.bias.data=torch.tensor(b).float().reshape(1,1).clone().detach().requires_grad_(True)\n",
    "\n",
    "        with torch.no_grad(): \n",
    "                    predicted=model(inputs).data.numpy()\n",
    "\n",
    "        plt.plot(x_train,predicted,'--',alpha=0.5)\n",
    "    plt.plot(x_train,predicted,'--',label='Predictions',alpha=0.5)    \n",
    "    plt.show()\n",
    "    time.sleep(3)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790fad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
